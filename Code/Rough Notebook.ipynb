{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ed4122",
   "metadata": {},
   "source": [
    "## Testing Windowed GAF Dataset on simple CNN-RNN-DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e87253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(168,168,1)),  \n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3,\n",
    "                      strides=1,\n",
    "                      activation=\"relu\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Reshape((-1, 64)), # Reshape the output to a 3D tensor\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c35d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Set the optimizer \n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "# Set the training parameters\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd961d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(gaf_dataset,epochs=5) # bad idea: it took ETA: 26:03:08 for first epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8818a03",
   "metadata": {},
   "source": [
    "## Test Generator for GAF Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_GAF_generator(series, window_size, batch_size, shuffle_buffer, stride):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=stride, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    gasf = GramianAngularField(method='summation')\n",
    "    \n",
    "    gaf_outputs = []\n",
    "    for window in dataset:\n",
    "        window = window.numpy()\n",
    "        gaf_transformed = gasf.fit_transform(window[:-1].reshape(1, -1))\n",
    "        gaf_outputs.append((tf.convert_to_tensor(gaf_transformed[0]), tf.convert_to_tensor(window[-1])))\n",
    "\n",
    "    return gaf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf_outputs = windowed_GAF_generator(series, window_size, batch_size, shuffle_buffer, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d03cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gaf_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17339664",
   "metadata": {},
   "source": [
    "## Testing Windowed GAF Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23eb0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_GAF(series, window_size, batch_size, shuffle_buffer, stride):\n",
    "\n",
    "    # Generate a TF Dataset from the series values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    \n",
    "    # Window the data but only take those with the specified size (note stride argument)\n",
    "    dataset = dataset.window(window_size + 1, shift=stride, drop_remainder=True)\n",
    "    \n",
    "    # Flatten the windows by putting its elements in a single batch\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    \n",
    "    # Create a GAF transformer\n",
    "    gasf = GramianAngularField(method='summation')\n",
    "    \n",
    "    # Create a function to apply GAF transformation\n",
    "    def apply_gaf(window):\n",
    "        return tf.numpy_function(\n",
    "            lambda win: (gasf.fit_transform(win[:-1].reshape(1, -1)), win[-1]),\n",
    "            [window],\n",
    "            (tf.float32, tf.float32),\n",
    "        )\n",
    "\n",
    "    # Create tuples with features and labels (Applying GAF transform on each window)\n",
    "    dataset = dataset.map(apply_gaf)\n",
    "    \n",
    "    # Shuffle the windows\n",
    "    #dataset = dataset.shuffle(shuffle_buffer)\n",
    "    \n",
    "    # Create batches of windows\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_windowed_GAF = windowed_GAF(series_train, window_size, batch_size, shuffle_buffer_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e54e4e",
   "metadata": {},
   "source": [
    "## Learning Rate Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8299063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial weights\n",
    "init_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde64c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate scheduler\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "# Set the training parameters\n",
    "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset_windowed, epochs=100, callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ac913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate array\n",
    "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set the grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the loss in log scale\n",
    "plt.semilogx(lrs, history.history[\"loss\"])\n",
    "\n",
    "# Increase the tickmarks size\n",
    "plt.tick_params('both', length=10, width=1, which='both')\n",
    "\n",
    "# Set the plot boundaries\n",
    "plt.axis([1e-8, 1e-3, 0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57895ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset states generated by Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Reset the weights\n",
    "model.set_weights(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff8f44",
   "metadata": {},
   "source": [
    "## Testing GAF Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea, define sampling windows first, then apply GAF transform to each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the GAF transformation to the training and validation sets\n",
    "gasf_data_train = gasf.fit_transform(series_train.reshape(1, -1))[0]\n",
    "gasf_data_valid = gasf.fit_transform(series_valid.reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset_2d(series, window_size, batch_size, shuffle_buffer):\n",
    "    n_samples = series.shape[1] - window_size\n",
    "    X = np.empty((n_samples, window_size, series.shape[0]))\n",
    "    y = np.empty(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        X[i] = series[:, i:i + window_size].T\n",
    "        y[i] = series[:, i + window_size]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7fe05",
   "metadata": {},
   "source": [
    "## h-step forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4449344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_step_forecast(model, series, window_size, n_forecast):\n",
    "    y = series\n",
    "    x_pred = y[-window_size - 1: - 1].reshape(1, window_size, 1)\n",
    "    y_pred = y[-1].reshape(1, 1, 1)\n",
    "\n",
    "    y_future = []\n",
    "    for i in range(n_forecast):\n",
    "        x_pred = np.append(x_pred[:, 1:, :], y_pred, axis=1)\n",
    "        y_pred = model.predict(x_pred).reshape(1, 1, 1)\n",
    "        y_future.append(y_pred.flatten().tolist())\n",
    "\n",
    "    y_future = np.array(y_future)\n",
    "\n",
    "    return y_future\n",
    "\n",
    "h_step = 1000\n",
    "y_future = h_step_forecast(model, series, window_size, n_forecast = h_step).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623461fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend initial forecast with h_step_forecast output\n",
    "extended = np.concatenate((forecast,y_future))\n",
    "extended_time = np.concatenate((time_valid,np.arange(time_valid[-1]+1,time_valid[-1]+1+h_step)))\n",
    "extended_series = np.concatenate((series_valid,np.zeros(h_step)))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_valid, series_valid, label=\"Actual\")\n",
    "plt.plot(extended_time, extended, label=\"Predicted\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85451071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOOMING INTO LAST 10% OF DATA POINTS\n",
    "zoom_start_idx = int(0.90 * len(time_valid))\n",
    "\n",
    "zoom_time_valid = time_valid[zoom_start_idx:]\n",
    "zoom_series_valid = series_valid[zoom_start_idx:]\n",
    "zoom_extended_time = extended_time[zoom_start_idx:]\n",
    "zoom_forecast = extended[zoom_start_idx:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(zoom_time_valid, zoom_series_valid, label=\"Actual\")\n",
    "plt.plot(zoom_extended_time, zoom_forecast, label=\"Predicted\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23ba1e",
   "metadata": {},
   "source": [
    "## LSTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5806c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from lstnet_tensorflow.layers import SkipGRU\n",
    "from lstnet_tensorflow.utils import get_training_sequences\n",
    "\n",
    "class LSTNet():\n",
    "\n",
    "    def __init__(self,\n",
    "                 y,\n",
    "                 forecast_period,\n",
    "                 lookback_period,\n",
    "                 filters=100,\n",
    "                 kernel_size=3,\n",
    "                 gru_units=100,\n",
    "                 skip_gru_units=50,\n",
    "                 skip=1,\n",
    "                 lags=1,\n",
    "                 dropout=0,\n",
    "                 regularizer='L2',\n",
    "                 regularization_factor=0.01):\n",
    "\n",
    "        '''\n",
    "        Implementation of multivariate time series forecasting model introduced in Lai, G., Chang, W. C., Yang, Y.,\n",
    "        & Liu, H. (2018). Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks. In \"The 41st\n",
    "        International ACM SIGIR Conference on Research & Development in Information Retrieval\" (SIGIR '18).\n",
    "        Association for Computing Machinery, New York, NY, USA, 95â€“104. \n",
    "\n",
    "        Parameters:\n",
    "        __________________________________\n",
    "        y: np.array.\n",
    "            Time series, array with shape (n_samples, n_targets) where n_samples is the length of the time series\n",
    "            and n_targets is the number of time series.\n",
    "\n",
    "        forecast_period: int.\n",
    "            Number of future time steps to forecast.\n",
    "\n",
    "        lookback_period: int.\n",
    "            Number of past time steps to use as input.\n",
    "\n",
    "        filters: int.\n",
    "            Number of filters (or channels) of the convolutional layer.\n",
    "\n",
    "        kernel_size: int.\n",
    "            Kernel size of the convolutional layer.\n",
    "\n",
    "        gru_units: list.\n",
    "            Hidden units of GRU layer.\n",
    "\n",
    "        skip_gru_units: list.\n",
    "            Hidden units of Skip GRU layer.\n",
    "\n",
    "        skip: int.\n",
    "            Number of skipped hidden cells in the Skip GRU layer.\n",
    "\n",
    "        lags: int.\n",
    "            Number of autoregressive lags.\n",
    "\n",
    "        dropout: float.\n",
    "            Dropout rate.\n",
    "\n",
    "        regularizer: str.\n",
    "            Regularizer, either 'L1', 'L2' or 'L1L2'.\n",
    "\n",
    "        regularization_factor: float.\n",
    "            Regularization factor.\n",
    "        '''\n",
    "\n",
    "        # Normalize the targets.\n",
    "        y_min, y_max = np.min(y, axis=0), np.max(y, axis=0)\n",
    "        y = (y - y_min) / (y_max - y_min)\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "\n",
    "        # Extract the input sequences and output values.\n",
    "        self.X, self.Y = get_training_sequences(y, lookback_period)\n",
    "\n",
    "        # Save the inputs.\n",
    "        self.y = y\n",
    "        self.n_samples = y.shape[0]\n",
    "        self.n_targets = y.shape[1]\n",
    "        self.n_lookback = lookback_period\n",
    "        self.n_forecast = forecast_period\n",
    "\n",
    "        # Build and save the model.\n",
    "        self.model = build_fn(\n",
    "            self.n_targets,\n",
    "            self.n_lookback,\n",
    "            filters,\n",
    "            kernel_size,\n",
    "            gru_units,\n",
    "            skip_gru_units,\n",
    "            skip,\n",
    "            lags,\n",
    "            dropout,\n",
    "            regularizer,\n",
    "            regularization_factor)\n",
    "\n",
    "    def fit(self,\n",
    "            loss='mse',\n",
    "            learning_rate=0.001,\n",
    "            batch_size=32,\n",
    "            epochs=100,\n",
    "            validation_split=0,\n",
    "            verbose=1):\n",
    "\n",
    "        '''\n",
    "        Train the model.\n",
    "\n",
    "        Parameters:\n",
    "        __________________________________\n",
    "        loss: str, function.\n",
    "            Loss function, see https://www.tensorflow.org/api_docs/python/tf/keras/losses.\n",
    "\n",
    "        learning_rate: float.\n",
    "            Learning rate.\n",
    "\n",
    "        batch_size: int.\n",
    "            Batch size.\n",
    "\n",
    "        epochs: int.\n",
    "            Number of epochs.\n",
    "\n",
    "        validation_split: float.\n",
    "            Fraction of the training data to be used as validation data, must be between 0 and 1.\n",
    "\n",
    "        verbose: int.\n",
    "            Verbosity mode: 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "        '''\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=loss,\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            x=self.X,\n",
    "            y=self.Y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    def forecast(self, y):\n",
    "    \n",
    "        '''\n",
    "        Generate the forecasts.\n",
    "        \n",
    "        Parameters:\n",
    "        __________________________________\n",
    "        y: np.array.\n",
    "            Past values of the time series.\n",
    "\n",
    "        Returns:\n",
    "        __________________________________\n",
    "        df: pd.DataFrame.\n",
    "            Data frame including the actual values of the time series and the forecasts.\n",
    "        '''\n",
    "\n",
    "        # Normalize the targets.\n",
    "        y = (y - self.y_min) / (self.y_max - self.y_min)\n",
    "        \n",
    "        # Generate the multi-step forecasts.\n",
    "        x_pred = y[- self.n_lookback - 1: - 1, :].reshape(1, self.n_lookback, self.n_targets)   # Last observed input sequence.\n",
    "        y_pred = y[-1:, :].reshape(1, 1, self.n_targets)                                        # Last observed target value.\n",
    "        y_future = []                                                                           # Future target values.\n",
    "        \n",
    "        for i in range(self.n_forecast):\n",
    "\n",
    "            # Feed the last forecast back to the model as an input.\n",
    "            x_pred = np.append(x_pred[:, 1:, :], y_pred, axis=1)\n",
    "\n",
    "            # Generate the next forecast.\n",
    "            y_pred = self.model(x_pred).numpy().reshape(1, 1, self.n_targets)\n",
    "\n",
    "            # Save the forecast.\n",
    "            y_future.append(y_pred.flatten().tolist())\n",
    "\n",
    "        y_future = np.array(y_future)\n",
    "\n",
    "        # Organize the forecasts in a data frame.\n",
    "        columns = ['time_idx']\n",
    "        columns.extend(['actual_' + str(i + 1) for i in range(self.n_targets)])\n",
    "        columns.extend(['predicted_' + str(i + 1) for i in range(self.n_targets)])\n",
    "\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df['time_idx'] = np.arange(self.n_samples + self.n_forecast)\n",
    "\n",
    "        for i in range(self.n_targets):\n",
    "\n",
    "            df['actual_' + str(i + 1)].iloc[: - self.n_forecast] = \\\n",
    "                self.y_min[i] + (self.y_max[i] - self.y_min[i]) * self.y[:, i]\n",
    "\n",
    "            df['predicted_' + str(i + 1)].iloc[- self.n_forecast:] = \\\n",
    "                self.y_min[i] + (self.y_max[i] - self.y_min[i]) * y_future[:, i]\n",
    "\n",
    "        # Return the data frame.\n",
    "        return df.astype(float)\n",
    "\n",
    "\n",
    "def build_fn(n_targets,\n",
    "             n_lookback,\n",
    "             filters,\n",
    "             kernel_size,\n",
    "             gru_units,\n",
    "             skip_gru_units,\n",
    "             skip,\n",
    "             lags,\n",
    "             dropout,\n",
    "             regularizer,\n",
    "             regularization_factor):\n",
    "\n",
    "    '''\n",
    "    Build the model, see Section 3 in the LSTNet paper.\n",
    "\n",
    "    Parameters:\n",
    "    __________________________________\n",
    "    n_targets: int.\n",
    "        Number of time series.\n",
    "\n",
    "    n_lookback: int.\n",
    "        Number of past time steps to use as input.\n",
    "\n",
    "    filters: int.\n",
    "        Number of filters (or channels) of the convolutional layer.\n",
    "\n",
    "    kernel_size: int.\n",
    "        Kernel size of the convolutional layer.\n",
    "\n",
    "    gru_units: list.\n",
    "        Hidden units of GRU layer.\n",
    "\n",
    "    skip_gru_units: list.\n",
    "        Hidden units of Skip GRU layer.\n",
    "\n",
    "    skip: int.\n",
    "        Number of skipped hidden cells in the Skip GRU layer.\n",
    "\n",
    "    lags: int.\n",
    "        Number of autoregressive lags.\n",
    "\n",
    "    dropout: float.\n",
    "        Dropout rate.\n",
    "\n",
    "    regularizer: str.\n",
    "        Regularizer, either 'L1', 'L2' or 'L1L2'.\n",
    "\n",
    "    regularization_factor: float.\n",
    "        Regularization factor.\n",
    "    '''\n",
    "\n",
    "    # Inputs.\n",
    "    x = tf.keras.layers.Input(shape=(n_lookback, n_targets))\n",
    "\n",
    "    # Convolutional component, see Section 3.2 in the LSTNet paper.\n",
    "    c = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(x)\n",
    "    c = tf.keras.layers.Dropout(rate=dropout)(c)\n",
    "\n",
    "    # Recurrent component, see Section 3.3 in the LSTNet paper.\n",
    "    r = tf.keras.layers.GRU(units=gru_units, activation='relu')(c)\n",
    "    r = tf.keras.layers.Dropout(rate=dropout)(r)\n",
    "\n",
    "    # Recurrent-skip component, see Section 3.4 in the LSTNet paper.\n",
    "    s = SkipGRU(units=skip_gru_units, activation='relu', return_sequences=True)(c)\n",
    "    s = tf.keras.layers.Dropout(rate=dropout)(s)\n",
    "    s = tf.keras.layers.Lambda(function=lambda x: x[:, - skip:, :])(s)\n",
    "    s = tf.keras.layers.Reshape(target_shape=(s.shape[1] * s.shape[2],))(s)\n",
    "    d = tf.keras.layers.Concatenate(axis=1)([r, s])\n",
    "    d = tf.keras.layers.Dense(units=n_targets, kernel_regularizer=kernel_regularizer(regularizer, regularization_factor))(d)\n",
    "\n",
    "    # Autoregressive component, see Section 3.6 in the LSTNet paper.\n",
    "    l = tf.keras.layers.Flatten()(x[:, - lags:, :])\n",
    "    l = tf.keras.layers.Dense(units=n_targets, kernel_regularizer=kernel_regularizer(regularizer, regularization_factor))(l)\n",
    "\n",
    "    # Outputs.\n",
    "    y = tf.keras.layers.Add()([d, l])\n",
    "\n",
    "    return tf.keras.models.Model(x, y)\n",
    "\n",
    "\n",
    "def kernel_regularizer(regularizer, regularization_factor):\n",
    "\n",
    "    '''\n",
    "    Define the kernel regularizer.\n",
    "\n",
    "    Parameters:\n",
    "    __________________________________\n",
    "    regularizer: str.\n",
    "        Regularizer, either 'L1', 'L2' or 'L1L2'.\n",
    "\n",
    "    regularization_factor: float.\n",
    "        Regularization factor.\n",
    "    '''\n",
    "\n",
    "    if regularizer == 'L1':\n",
    "        return tf.keras.regularizers.L1(l1=regularization_factor)\n",
    "\n",
    "    elif regularizer == 'L2':\n",
    "        return tf.keras.regularizers.L2(l2=regularization_factor)\n",
    "\n",
    "    elif regularizer == 'L1L2':\n",
    "        return tf.keras.regularizers.L1L2(l1=regularization_factor, l2=regularization_factor)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Undefined regularizer {}.'.format(regularizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SkipGRU(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 p=1,\n",
    "                 activation='relu',\n",
    "                 return_sequences=False,\n",
    "                 return_state=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        '''\n",
    "        Recurrent-skip layer, see Section 3.4 in the LSTNet paper.\n",
    "        \n",
    "        Parameters:\n",
    "        __________________________________\n",
    "        units: int.\n",
    "            Number of hidden units of the GRU cell.\n",
    "\n",
    "        p: int.\n",
    "            Number of skipped hidden cells.\n",
    "\n",
    "        activation: str, function.\n",
    "            Activation function, see https://www.tensorflow.org/api_docs/python/tf/keras/activations.\n",
    "\n",
    "        return_sequences: bool.\n",
    "            Whether to return the last output or the full sequence.\n",
    "\n",
    "        return_state: bool.\n",
    "            Whether to return the last state in addition to the output.\n",
    "\n",
    "        **kwargs: See https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell.\n",
    "        '''\n",
    "\n",
    "        if p < 1:\n",
    "            raise ValueError('The number of skipped hidden cells cannot be less than 1.')\n",
    "\n",
    "        self.units = units\n",
    "        self.p = p\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        self.timesteps = None\n",
    "        self.cell = tf.keras.layers.GRUCell(units=units, activation=activation, **kwargs)\n",
    "\n",
    "        super(SkipGRU, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        if self.timesteps is None:\n",
    "            self.timesteps = input_shape[1]\n",
    "\n",
    "            if self.p > self.timesteps:\n",
    "                raise ValueError('The number of skipped hidden cells cannot be greater than the number of timesteps.')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        '''\n",
    "        Parameters:\n",
    "        __________________________________\n",
    "        inputs: tf.Tensor.\n",
    "            Layer inputs, 2-dimensional tensor with shape (n_samples, filters) where n_samples is the batch size\n",
    "            and filters is the number of channels of the convolutional layer.\n",
    "\n",
    "        Returns:\n",
    "        __________________________________\n",
    "        outputs: tf.Tensor.\n",
    "            Layer outputs, 2-dimensional tensor with shape (n_samples, units) if return_sequences == False,\n",
    "            3-dimensional tensor with shape (n_samples, n_lookback, units) if return_sequences == True where\n",
    "            n_samples is the batch size, n_lookback is the number of past time steps used as input and units\n",
    "            is the number of hidden units of the GRU cell.\n",
    "\n",
    "        states: tf.Tensor.\n",
    "            Hidden states, 2-dimensional tensor with shape (n_samples, units) where n_samples is the batch size\n",
    "            and units is the number of hidden units of the GRU cell.\n",
    "        '''\n",
    "\n",
    "        outputs = tf.TensorArray(\n",
    "            element_shape=(inputs.shape[0], self.units),\n",
    "            size=self.timesteps,\n",
    "            dynamic_size=False,\n",
    "            dtype=tf.float32,\n",
    "            clear_after_read=False\n",
    "        )\n",
    "\n",
    "        states = tf.TensorArray(\n",
    "            element_shape=(inputs.shape[0], self.units),\n",
    "            size=self.timesteps,\n",
    "            dynamic_size=False,\n",
    "            dtype=tf.float32,\n",
    "            clear_after_read=False\n",
    "        )\n",
    "\n",
    "        initial_states = tf.zeros(\n",
    "            shape=(tf.shape(inputs)[0], self.units),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        for t in tf.range(self.timesteps):\n",
    "\n",
    "            if t < self.p:\n",
    "                output, state = self.cell(\n",
    "                    inputs=inputs[:, t, :],\n",
    "                    states=initial_states\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                output, state = self.cell(\n",
    "                    inputs=inputs[:, t, :],\n",
    "                    states=states.read(t - self.p)\n",
    "                )\n",
    "\n",
    "            outputs = outputs.write(index=t, value=output)\n",
    "            states = states.write(index=t, value=state)\n",
    "\n",
    "        outputs = tf.transpose(outputs.stack(), [1, 0, 2])\n",
    "        states = tf.transpose(states.stack(), [1, 0, 2])\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            outputs = outputs[:, -1, :]\n",
    "\n",
    "        if self.return_state:\n",
    "            states = states[:, -1, :]\n",
    "            return outputs, states\n",
    "\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_training_sequences(y, n_lookback):\n",
    "\n",
    "    '''\n",
    "    Split the time series into input sequences and output values. These are used for training the model.\n",
    "    See Sections 3.1 and 3.8 in the LSTNet paper.\n",
    "\n",
    "    Parameters:\n",
    "    __________________________________\n",
    "    y: np.array.\n",
    "        Time series, array with shape (n_samples, n_targets) where n_samples is the length of the time\n",
    "        series and n_targets is the number of time series.\n",
    "\n",
    "    n_lookback: int.\n",
    "        The number of past time steps used as input.\n",
    "\n",
    "    Returns:\n",
    "    __________________________________\n",
    "    X: np.array.\n",
    "        Input sequences, array with shape (n_samples - n_lookback, n_lookback, n_targets).\n",
    "\n",
    "    Y: np.array.\n",
    "        Output values, array with shape (n_samples - n_lookback, n_targets).\n",
    "    '''\n",
    "\n",
    "    X = np.zeros((y.shape[0], n_lookback, y.shape[1]))\n",
    "    Y = np.zeros((y.shape[0], y.shape[1]))\n",
    "\n",
    "    for i in range(n_lookback, y.shape[0]):\n",
    "\n",
    "        X[i, :, :] = y[i - n_lookback: i, :]\n",
    "        Y[i, :] = y[i, :]\n",
    "\n",
    "    X = X[n_lookback:, :, :]\n",
    "    Y = Y[n_lookback:, :]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from lstnet_tensorflow.model import LSTNet\n",
    "from lstnet_tensorflow.plots import plot\n",
    "\n",
    "# Generate some time series\n",
    "N = 500\n",
    "t = np.linspace(0, 1, N)\n",
    "e = np.random.multivariate_normal(mean=np.zeros(3), cov=np.eye(3), size=N)\n",
    "a = 10 + 10 * t + 10 * np.cos(2 * np.pi * (10 * t - 0.5)) + 1 * e[:, 0]\n",
    "b = 20 + 20 * t + 20 * np.cos(2 * np.pi * (20 * t - 0.5)) + 2 * e[:, 1]\n",
    "c = 30 + 30 * t + 30 * np.cos(2 * np.pi * (30 * t - 0.5)) + 3 * e[:, 2]\n",
    "y = np.hstack([a.reshape(-1, 1), b.reshape(-1, 1), c.reshape(-1, 1)])\n",
    "\n",
    "# Fit the model\n",
    "model = LSTNet(\n",
    "    y=y,\n",
    "    forecast_period=100,\n",
    "    lookback_period=200,\n",
    "    kernel_size=3,\n",
    "    filters=4,\n",
    "    gru_units=4,\n",
    "    skip_gru_units=3,\n",
    "    skip=50,\n",
    "    lags=100,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    loss='mse',\n",
    "    learning_rate=0.01,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Generate the forecasts\n",
    "df = model.forecast(y=y)\n",
    "\n",
    "# Plot the forecasts\n",
    "fig = plot(df=df)\n",
    "fig.write_image('results.png', scale=4, height=900, width=700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
